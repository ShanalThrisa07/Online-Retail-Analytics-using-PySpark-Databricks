# Online-Retail-Analytics-using-PySpark-Databricks

## Project Overview

This project demonstrates an end-to-end data analytics and data engineering workflow using PySpark on Databricks, following the Medallion Architecture (Bronze → Silver → Gold).

The dataset represents historical online retail transactions and is used to build clean analytical datasets and business-ready customer metrics such as Customer Lifetime Value (CLV).

The project is designed to reflect real-world data engineering practices and is suitable for resume, GitHub, and interview discussions.

## Project Objectives

Ingest raw retail transaction data into Databricks using PySpark

Handle real-world data quality issues (schema inconsistencies, invalid data types, nulls)

Design Silver-layer cleaned datasets for analytics

Build Gold-layer business aggregations (Customer Lifetime Value)

Export processed datasets for reproducibility and version control

## Architecture Used (Medallion Architecture)

**Bronze Layer – Raw Data**

Source: Original Excel dataset with multiple sheets (2009–2011 transactions)

Converted Excel sheets into individual CSV files for ingestion

Minimal transformation applied

**Silver Layer – Cleaned & Standardized Data**

Standardized column names (Delta-compatible)

Converted string-based numeric fields to proper numeric types

Parsed invoice timestamps using explicit formats

Handled malformed records and null values safely

Derived revenue at transaction level

**Gold Layer – Business Aggregations**

Aggregated customer-level metrics

Calculated Customer Lifetime Value (CLV)

Computed total number of orders using distinct invoice counts

Sorted customers by lifetime value for business prioritization

## Dataset Information

**Dataset Name**: Online Retail II

**Time Period**: 2009–2011

**Grain**: Transaction-level retail data

**Key Columns**:

Invoice

InvoiceDate

CustomerID

StockCode

Quantity

Price

Country

## Technologies Used

Apache Spark (PySpark)

Databricks Community Edition (No AWS / cloud services used)

Delta Lake concepts

Python

CSV file format for data exchange

## Key Business Metrics Built

**Customer Lifetime Value (CLV)**

Defined as total revenue generated by a customer over their lifetime

Calculated as:

CLV = Sum of Revenue per Customer

**Total Orders**

Count of distinct invoices per customer

Prevents double-counting due to multiple line items per invoice

## Insights Enabled

Identification of high-value customers

Understanding revenue concentration across customers

Foundation for customer segmentation and retention strategies

## Conclusion

This project demonstrates a complete PySpark-based analytics workflow, starting from raw online retail transaction data and progressing through structured data cleaning and business-focused aggregation using the **Medallion Architecture (Bronze → Silver → Gold)**.

By deliberately separating data ingestion, standardization, and business aggregation layers, the project mirrors real-world data engineering practices and emphasizes **data quality, scalability, and clarity of design**. The Silver layer ensures analytical correctness by handling schema inconsistencies, malformed data, and type conversions, while the Gold layer transforms cleaned data into meaningful business metrics.

The final **Customer Lifetime Value (CLV)** Gold dataset converts transaction-level data into actionable customer insights, enabling use cases such as customer prioritization, retention analysis, and revenue contribution assessment. This reflects a clear understanding of data grain, aggregation logic, and business alignment.






Readiness for downstream BI tools and dashboards
